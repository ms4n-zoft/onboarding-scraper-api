# Product Scraper Engine

Production-ready web scraping and analysis engine powered by Azure OpenAI, with async job processing, real-time event streaming, and comprehensive API.

## Features

- ğŸ¤– **AI-Powered Analysis**: Uses Azure OpenAI (GPT-4) for intelligent content extraction
- ğŸ“¡ **Real-Time Streaming**: Server-Sent Events (SSE) for live job progress
- âš¡ **Background Processing**: Redis Queue (RQ) for scalable async job handling
- ğŸ”„ **Event Persistence**: 24-hour TTL on events and results
- ğŸ“Š **Comprehensive API**: REST endpoints for job submission, status, and results
- ğŸ› ï¸ **Production Ready**: Systemd services, Docker support, monitoring, and more

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚â”€â”€â”€â”€â–¶â”‚  API Server â”‚â”€â”€â”€â”€â–¶â”‚    Redis    â”‚
â”‚  (Browser)  â”‚â—€â”€â”€â”€â”€â”‚  (FastAPI)  â”‚â—€â”€â”€â”€â”€â”‚   (Queue)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â–¼
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚ RQ Workers  â”‚
                                        â”‚  (N nodes)  â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Prerequisites

- Python 3.11+
- Redis (Upstash recommended for cloud deployment)
- Azure OpenAI resource with GPT-4 deployment

### Installation

1. **Clone and setup virtual environment**:

   ```bash
   git clone https://github.com/your-org/product-scraper-engine.git
   cd product-scraper-engine
   python3.11 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

2. **Install dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

3. **Configure environment variables**:

   ```bash
   cp .env.example .env
   # Edit .env with your credentials
   ```

   Required variables:

   - `AZURE_OPENAI_ENDPOINT`
   - `AZURE_OPENAI_API_KEY`
   - `AZURE_OPENAI_DEPLOYMENT_NAME`
   - `REDIS_URL`

   See [ENVIRONMENT_VARIABLES.md](ENVIRONMENT_VARIABLES.md) for complete reference.

4. **Verify setup**:
   ```bash
   python tests/test_worker_setup.py
   ```

## Usage

### CLI Mode

```bash
python -m src.main https://www.leadspace.com/
```

The script prints structured JSON with extracted details. Use `--out <path>` to persist the JSON to disk.

### FastAPI Server

To run the API server:

```bash
python -m uvicorn src.api:app --reload
```

The server will start on `http://localhost:8000`.

#### API Endpoints

**Health Check**

```
GET /health
```

**Synchronous Scrape (Blocking)**

```
POST /scrape
```

Request body:

```json
{
  "source_url": "https://www.leadspace.com/"
}
```

```bash
curl -X POST http://localhost:8000/scrape \
  -H "Content-Type: application/json" \
  -d '{"source_url": "https://www.leadspace.com/"}'
```

Response:

```json
{
  "success": true,
  "data": {
    "product_name": "...",
    "company_name": "...",
    "website": "...",
    "overview": "...",
    ...
  },
  "error": null
}
```

**Async Scrape (Background Job)**

```
POST /scrape/async
```

Submits a scraping job to the background queue and returns immediately.

Request body:

```json
{
  "source_url": "https://www.leadspace.com/"
}
```

Response:

```json
{
  "job_id": "abc123-def456-...",
  "status": "queued",
  "stream_url": "/jobs/abc123-def456-.../stream"
}
```

**Stream Job Events (Server-Sent Events)**

```
GET /jobs/{job_id}/stream
```

Stream real-time events from a background job using SSE.

```bash
curl -N http://localhost:8000/jobs/abc123-def456-.../stream
```

Events include: `start`, `reading`, `update`, `complete`, `error`

**Get Job Status**

```
GET /jobs/{job_id}/status
```

Get the current status and metadata of a background job.

Response:

```json
{
  "job_id": "abc123-def456-...",
  "status": "finished",
  "enqueued_at": "2025-11-05T12:00:00Z",
  "started_at": "2025-11-05T12:00:05Z",
  "ended_at": "2025-11-05T12:00:30Z",
  "position": null
}
```

**Get Job Result**

```
GET /jobs/{job_id}/result
```

Retrieve the result of a completed background job.

Response:

```json
{
  "job_id": "abc123-def456-...",
  "status": "finished",
  "result": {
    "product_name": "...",
    "company_name": "...",
    ...
  },
  "error": null
}
```

#### Interactive Documentation

Visit `http://localhost:8000/docs` for Swagger UI documentation or `http://localhost:8000/redoc` for ReDoc documentation.

### Background Workers (RQ)

For background job processing, you need to run RQ workers separately from the API server.

#### Starting a Worker

Basic usage:

```bash
python worker.py
```

With custom configuration:

```bash
# Custom worker name
RQ_WORKER_NAME=worker-1 python worker.py

# Burst mode (process all jobs then exit)
RQ_WORKER_BURST=true python worker.py
```

#### Worker Logs

Worker logs are stored in the `logs/` directory with daily rotation:

- `logs/worker_YYYY-MM-DD.log` - Daily log files (kept for 30 days)
- Console output with colored formatting

#### Production Deployment

For production, run multiple workers:

```bash
# Terminal 1 - API Server
uvicorn src.api:app --host 0.0.0.0 --port 8000 --workers 4

# Terminal 2 - Worker 1
RQ_WORKER_NAME=worker-1 python worker.py

# Terminal 3 - Worker 2
RQ_WORKER_NAME=worker-2 python worker.py

# Terminal 4 - Worker 3
RQ_WORKER_NAME=worker-3 python worker.py
```

#### Monitoring Workers

Check queue status:

```python
from redis import Redis
from rq import Queue

redis_conn = Redis.from_url("your-redis-url")
queue = Queue("scraper", connection=redis_conn)

print(f"Jobs in queue: {len(queue)}")
print(f"Failed jobs: {queue.failed_job_registry.count}")
```

Or use RQ's built-in dashboard:

```bash
pip install rq-dashboard
rq-dashboard --redis-url your-redis-url
```

Visit `http://localhost:9181` to see the dashboard.

## Testing

### Unit and Integration Tests

```bash
# Test Redis configuration
python tests/test_redis.py

# Test event emitter
python tests/test_redis_event_emitter.py

# Test worker setup
python tests/test_worker_setup.py

# Test API endpoints
python tests/test_async_endpoint.py
python tests/test_stream_endpoint.py
python tests/test_status_result_endpoints.py

# Integration test (requires worker running)
python tests/test_integration.py
```

### Manual Testing

```bash
# Terminal 1: Start worker
python worker.py

# Terminal 2: Submit a test job
python tests/test_worker_manual.py
```

## Documentation

- **[ENVIRONMENT_VARIABLES.md](docs/ENVIRONMENT_VARIABLES.md)** - Complete environment variable reference
- **[DEPLOYMENT.md](docs/DEPLOYMENT.md)** - Production deployment guide (systemd, Docker, Kubernetes)
- **[BACKEND_PHASE2_IMPLEMENTATION.md](docs/BACKEND_PHASE2_IMPLEMENTATION.md)** - Technical implementation details
- **[FRONTEND_API_DOCS.md](docs/FRONTEND_API_DOCS.md)** - API documentation for frontend integration
- **[PHASE2_COMPLETE.md](docs/PHASE2_COMPLETE.md)** - Phase 2 completion summary

## Project Structure

```
product-scraper-engine/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api.py                  # FastAPI application and endpoints
â”‚   â”œâ”€â”€ main.py                 # CLI entry point
â”‚   â”œâ”€â”€ dependencies.py         # Dependency injection (Redis, Queue)
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ analyzer.py         # Synchronous AI analyzer
â”‚   â”‚   â””â”€â”€ agentic_analyzer.py # Agentic AI analyzer
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ azure.py            # Azure OpenAI configuration
â”‚   â”‚   â””â”€â”€ redis.py            # Redis connection management
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â””â”€â”€ scraper_task.py     # RQ worker job functions
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ api.py              # API request/response models
â”‚   â”‚   â”œâ”€â”€ events.py           # SSE event models
â”‚   â”‚   â””â”€â”€ product.py          # Product data schemas
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â”œâ”€â”€ fetcher.py          # HTTP fetching utilities
â”‚   â”‚   â””â”€â”€ parser.py           # HTML parsing utilities
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ env.py              # Environment variable utilities
â”‚       â”œâ”€â”€ event_emitter.py    # Base event emitter
â”‚       â”œâ”€â”€ redis_event_emitter.py # Redis-backed event emitter
â”‚       â””â”€â”€ logging.py          # Logging configuration
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_redis.py           # Redis connection tests
â”‚   â”œâ”€â”€ test_redis_event_emitter.py # Event emitter tests
â”‚   â”œâ”€â”€ test_rq_job.py          # Job execution tests
â”‚   â”œâ”€â”€ test_async_endpoint.py  # Async endpoint tests
â”‚   â”œâ”€â”€ test_stream_endpoint.py # SSE streaming tests
â”‚   â”œâ”€â”€ test_status_result_endpoints.py # Status/result tests
â”‚   â”œâ”€â”€ test_worker_setup.py    # Worker configuration tests
â”‚   â”œâ”€â”€ test_worker_manual.py   # Manual worker testing
â”‚   â””â”€â”€ test_integration.py     # End-to-end integration test
â”œâ”€â”€ worker.py                   # RQ worker entry point
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ .env.example                # Environment variable template
```

## Troubleshooting

### Common Issues

**"Missing required environment variable: REDIS_URL"**

- Solution: Create a `.env` file with required variables (see `.env.example`)

**Worker not processing jobs**

- Check worker is running: `ps aux | grep worker.py`
- Verify Redis connection: `python tests/test_worker_setup.py`
- Check queue: Use RQ dashboard or check logs

**SSE stream not working**

- Ensure worker is running and processing the job
- Check browser console for connection errors
- Verify CORS settings if accessing from different origin

**Jobs failing with timeout**

- Increase `job_timeout` in job submission
- Check Azure OpenAI rate limits
- Verify network connectivity to Azure OpenAI

### Support

For issues and feature requests, please open an issue on GitHub.

## License

[Your License Here]

## Contributing

Contributions are welcome! Please read our contributing guidelines before submitting PRs.

## Roadmap

- [ ] Add Playwright for JavaScript-heavy sites
- [ ] Implement rate limiting on API endpoints
- [ ] Add job priority queues
- [ ] Support for bulk job submission
- [ ] Webhook notifications for job completion
- [ ] Enhanced error recovery and retry logic
- [ ] Support for custom extraction schemas
- [ ] Job scheduling and cron-like triggers
